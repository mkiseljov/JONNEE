{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired GCN\n",
    "\n",
    "Training of two Graph convolutional networks with coupled losses in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs:\n",
    "\n",
    "- currently only works for multiclass classification; maybe extend multilabel with classifier chains or try other tasks\n",
    "\n",
    "- unsupervised: try various loss functions\n",
    "\n",
    "- other architectures (deeper GCN, for example)\n",
    "\n",
    "- replace GCN with GAE?\n",
    "\n",
    "- try other loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import load_multiclass, accuracy\n",
    "from utils import get_dual\n",
    "from utils import sparse_mx_to_torch_sparse_tensor\n",
    "from utils import log_progress\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from models import GCN\n",
    "\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def f1(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels).data\n",
    "#     print(preds)\n",
    "#     print(f1_score(labels, preds))\n",
    "    return f1_score(labels.data, preds, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 20\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "n_hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 1e-2\n",
    "seed = 42\n",
    "weight_decay = 5e-4\n",
    "fastmode = True\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading other data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load multiclass data : make blog_catalog classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m       \u001b[31mreadme.txt\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/BlogCatalog-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`adj` - матрица смежности, исходного графа в coo формате\n",
    "\n",
    "`features` -  матрица\n",
    "\n",
    "`labels`\n",
    "\n",
    "`idx_train`\n",
    "\n",
    "`idx_val`\n",
    "\n",
    "`idx_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading blog_catalog dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_multiclass(path='../data/BlogCatalog-dataset/data/',\n",
    "                                                                      dataset='blog_catalog')\n",
    "\n",
    "\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = load_multiclass()\n",
    "features = torch.FloatTensor(np.identity(adj.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All unimported definitions\n",
    "\n",
    "def make_incidence_matrix(adj, edges, idx_train):\n",
    "    \"\"\"\n",
    "    Make a (V x E) vertex-edge incidence matrix\n",
    "    \"\"\"\n",
    "    row, col = [], []\n",
    "    for edge_id in edges:\n",
    "        a, b = edges[edge_id]\n",
    "        if a not in idx_train or b not in idx_train:\n",
    "            continue\n",
    "        row.append(a); col.append(edge_id)\n",
    "        row.append(b); col.append(edge_id)\n",
    "    arr = sp.coo_matrix(([1] * len(row), (row, col)), shape=(adj.shape[0], len(edges)))\n",
    "    return sparse_mx_to_torch_sparse_tensor(arr)\n",
    "\n",
    "def make_incidence_matrix_EV(adj, edges, idx_train):\n",
    "    \"\"\"\n",
    "    Make a (E x V) vertex-edge incidence matrix\n",
    "    \"\"\"\n",
    "    row, col = [], []\n",
    "    for edge_id in edges:\n",
    "        a, b = edges[edge_id]\n",
    "        if a not in idx_train or b not in idx_train:\n",
    "            continue\n",
    "        col.append(a); row.append(edge_id)\n",
    "        col.append(b); row.append(edge_id)\n",
    "    arr = sp.coo_matrix(([1] * len(row), (row, col)), shape=(len(edges), adj.shape[0]))\n",
    "    return sparse_mx_to_torch_sparse_tensor(arr)\n",
    "\n",
    "class SparseMM2_GCN(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Sparse x dense matrix multiplication with autograd support.\n",
    "\n",
    "    Implementation by Soumith Chintala:\n",
    "    https://discuss.pytorch.org/t/\n",
    "    does-pytorch-support-autograd-on-sparse-matrix/6156/7\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sparse):\n",
    "        super(SparseMM2_GCN, self).__init__()\n",
    "        self.sparse = sparse\n",
    "\n",
    "    def forward(self, dense):\n",
    "        return torch.mm(self.sparse.data, dense)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = None\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = torch.mm(self.sparse.data.t(), grad_output)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 46s, sys: 8.52 s, total: 6min 54s\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# in: adj - scipy sparse matrix\n",
    "# out: 1) dual adjacency matrix\n",
    "#  and 2) a dict of all edges\n",
    "\n",
    "# nx graph representation of the original graph\n",
    "gr = nx.from_scipy_sparse_matrix(adj).to_directed()\n",
    "edge_to_id = {edge: i for i, edge in enumerate(gr.edges())}\n",
    "edges = {i: edge for i, edge in enumerate(gr.edges())}\n",
    "\n",
    "row, col = [], []\n",
    "\n",
    "nodes = list(gr.nodes())\n",
    "for v in log_progress(nodes):\n",
    "    for n1, n2 in product(gr.neighbors(v), gr.neighbors(v)):\n",
    "        # {v, n1} and {v, n2} are (undirected) edges\n",
    "        row.append(edge_to_id[(v, n1)])\n",
    "        col.append(edge_to_id[(v, n2)])\n",
    "        row.append(edge_to_id[(v, n2)])\n",
    "        col.append(edge_to_id[(v, n1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 1min 4s, total: 3min 36s\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "del gr\n",
    "adj_d = sp.coo_matrix(([1] * len(row), (row, col)), \n",
    "                      shape=(len(edges), len(edges)))\n",
    "del row, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# at this point, adj_d is a scipy.sparse matrix\n",
    "adj_d = sparse_mx_to_torch_sparse_tensor(adj_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d times E x E times V = d times V\n",
    "commat = make_incidence_matrix_EV(adj, edges, idx_train.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_d = sparse_mx_to_torch_sparse_tensor(sp.eye(adj_d.shape[0]))\n",
    "features_d = torch.FloatTensor(np.identity(adj_d.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нам нужно дальше:\n",
    "\n",
    "adj, features\n",
    "\n",
    "adj_d, features_d\n",
    "\n",
    "labels\n",
    "\n",
    "Наборы индексов для сплита\n",
    "\n",
    "надо ли приближать на сплите\n",
    "\n",
    "commat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features = sparse_mx_to_torch_sparse_tensor(sp.eye(adj.shape[0]))\n",
    "# features = Variable(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversions\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "features, labels = Variable(features), Variable(labels)\n",
    "features_d, commat = Variable(features_d), Variable(commat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np = labels.data.numpy()\n",
    "edge_labels = Variable(torch.LongTensor( np.array([labels_np[a] == labels_np[b] \n",
    "                                                  for a, b in edges.values()], dtype='int') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 11258\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "⋮ \n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.LongTensor of size 13264]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do not do this, causes problems with torch.mm\n",
    "# adj   = Variable(adj)\n",
    "# adj_d = Variable(adj_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=n_hidden,\n",
    "            nclass=labels.max().data[0] + 1,\n",
    "            dropout=dropout)\n",
    "\n",
    "model_d = GCN(nfeat=features_d.shape[1],\n",
    "             nhid=n_hidden,\n",
    "             nclass=2,\n",
    "             dropout=dropout)\n",
    "\n",
    "# Optimizers\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "optimizer_d = optim.Adam(model_d.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multiclass_classification_combined_loss(output, labels, output_d, edge_labels, approx_err,\n",
    "                                            idx_train, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute negative log-likelihood loss for multiclass task\n",
    "    \"\"\"\n",
    "    orig_loss = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    dual_loss = F.nll_loss(output_d, edge_labels)\n",
    "    \n",
    "    return orig_loss + alpha * dual_loss + beta * approx_err\n",
    "\n",
    "\n",
    "        \n",
    "alpha = 1e-1\n",
    "beta = 1e-2\n",
    "alpha = Variable(torch.FloatTensor([alpha]))\n",
    "beta = Variable(torch.FloatTensor([beta]))\n",
    "\n",
    "\n",
    "def train(epoch, verbose=True):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    model_d.train()\n",
    "    optimizer_d.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    output_d = model_d(features_d, adj_d)\n",
    "\n",
    "    approx_err = ((SparseMM2_GCN(commat)(model.gc1.weight) - model_d.gc1.weight) ** 2).sum()\n",
    "\n",
    "    loss_train = multiclass_classification_combined_loss(output, labels, output_d, edge_labels,\n",
    "                                                        approx_err, idx_train, alpha, beta)\n",
    "\n",
    "    \n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    if verbose:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.data[0]),\n",
    "              'acc_train: {:.4f}'.format(acc_train.data[0]),\n",
    "              'loss_val: {:.4f}'.format(loss_val.data[0]),\n",
    "              'acc_val: {:.4f}'.format(acc_val.data[0]),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    f1_test = f1(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data[0]),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data[0]),\n",
    "          \"f1= {:.4f}\".format(f1_test))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "\n",
    "for epoch in range(300):\n",
    "    train(epoch, verbose=False)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение с базовой моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Base model and optimizer\n",
    "\n",
    "model_base = GCN(nfeat=features.shape[1],\n",
    "                nhid=n_hidden,\n",
    "                nclass=labels.max().data[0] + 1,\n",
    "                dropout=dropout)\n",
    "\n",
    "optimizer_base = optim.Adam(model_base.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def train_base(epoch, verbose=True):\n",
    "    t = time.time()\n",
    "    model_base.train()\n",
    "    optimizer_base.zero_grad()\n",
    "    output = model_base(features, adj)\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer_base.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model_base.eval()\n",
    "        output = model_base(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    if verbose:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.data[0]),\n",
    "              'acc_train: {:.4f}'.format(acc_train.data[0]),\n",
    "              'loss_val: {:.4f}'.format(loss_val.data[0]),\n",
    "              'acc_val: {:.4f}'.format(acc_val.data[0]),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test_base():\n",
    "    model_base.eval()\n",
    "    output = model_base(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    f1_test = f1(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data[0]),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data[0]),\n",
    "          \"f1= {:.4f}\".format(f1_test))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_base(epoch, verbose=False)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test_base()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making features (not sparse, use original layers) with diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_graph, dotdict, encode_onehot\n",
    "\n",
    "def get_data_with_diffusion_features(args_p, args_d, subgraph={'num': None, 'seed': 0}, dataset='cora'):    \n",
    "    path_to_edgelist = args_p.dataset_str\n",
    "\n",
    "    args_p.input = 'temp'\n",
    "    args_p.output = 'temp_out'\n",
    "    args_d.input = 'temp_d'\n",
    "    args_d.output = 'temp_d_out'\n",
    "\n",
    "    # set the seed for labels and train-test split\n",
    "    np.random.seed(subgraph['seed'])\n",
    "    \n",
    "    # save edgelists to temp locations\n",
    "    if dataset=='bc':\n",
    "        graph = load_graph(path_to_edgelist, subgraph=subgraph['num'], seed=subgraph['seed'])\n",
    "        bc_classes = '../data/BlogCatalog-dataset/data/blog_catalog.classes'\n",
    "        # we have this nodelist, so we can select labels for these nodes\n",
    "        labels = np.array([np.random.choice(\n",
    "                            np.where(\n",
    "                                np.array( list(map(int, line.split())) )\n",
    "                                )[0],\n",
    "                            size=1)\n",
    "                            for line in open(bc_classes)],\n",
    "                            dtype='int')\n",
    "            \n",
    "    if dataset=='cora':\n",
    "        idx_features_labels = np.genfromtxt(\"../data/cora/cora.content\",\n",
    "                                        dtype=np.dtype(str))\n",
    "        idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "        idx_map = {j: i for i, j in enumerate(idx)}\n",
    "        edges_unordered = np.genfromtxt(\"../data/cora/cora.cites\",\n",
    "                                            dtype=np.int32)\n",
    "        edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                         dtype=np.int32).reshape(edges_unordered.shape)\n",
    "        graph = nx.from_edgelist(edges)\n",
    "        idx_features_labels = np.genfromtxt(\"../data/cora/cora.content\",\n",
    "                                        dtype=np.dtype(str))\n",
    "        labels = encode_onehot(idx_features_labels[:, -1])\n",
    "        labels = np.where(labels)[1]\n",
    "    \n",
    "    ## common part\n",
    "    \n",
    "    nodelist = sorted(graph.nodes())\n",
    "\n",
    "    m = {i: n for i, n in enumerate(nodelist)}\n",
    "    inv_m = {n: i for i, n in enumerate(nodelist)}\n",
    "    adj_mat = sp.csr_matrix(nx.to_numpy_matrix(graph, nodelist))\n",
    "\n",
    "    # train-test split\n",
    "    n = len(nodelist)\n",
    "    rng = list(range(n))\n",
    "    np.random.shuffle(rng)\n",
    "    idx_train = rng[:int(n*0.6)]\n",
    "    idx_val = rng[int(n*0.6):int(n*0.8)]\n",
    "    idx_test = rng[int(n*0.8):]\n",
    "\n",
    "    # save edgelist graph.edges()\n",
    "    with open(args_p.input, 'w') as f:\n",
    "         f.write(\"\\n\".join(str(inv_m[a]) + \",\" + str(inv_m[b]) for (a, b) in graph.edges()))\n",
    "                                        #if inv_m[a] in idx_train and inv_m[b] in idx_train))\n",
    "    \n",
    "    dual_graph = nx.line_graph(graph)\n",
    "    edgelist = sorted(dual_graph.nodes())\n",
    "    dual_nodes = {e: i for i, e in enumerate(edgelist)}\n",
    "    # save edgelist dual_graph.edges()\n",
    "    with open(args_d.input, 'w') as f:\n",
    "         f.write(\"\\n\".join(str(dual_nodes[a]) + \",\" + str(dual_nodes[b]) for (a, b) in dual_graph.edges()))\n",
    "                                                                #if a[0] in b and inv_m[a[0]] in idx_train\\\n",
    "                                                                #or a[1] in b and inv_m[a[1]] in idx_train))\n",
    "    \n",
    "    adj_dual = sp.csr_matrix(nx.to_numpy_matrix(dual_graph, edgelist))        \n",
    "    \n",
    "    ###############################################################################\n",
    "    \n",
    "    args_p.dimensions = graph.number_of_nodes()\n",
    "    args_d.dimensions = dual_graph.number_of_nodes()\n",
    "    \n",
    "    # Obtain features\n",
    "    #----- Primal\n",
    "    print('\\n\\tStage 1: training for features')\n",
    "    emb_ = get_diff2vec_embeddings(args_p,  verbose=0)\n",
    "    emb = np.zeros(shape=(graph.number_of_nodes(), args_p.dimensions))\n",
    "    i2w = emb_.index2word\n",
    "    for w in i2w:\n",
    "        emb[int(w), :] = emb_[w]\n",
    "\n",
    "\n",
    "    print('\\n\\n\\n')\n",
    "    #----- Dual\n",
    "    print('\\n\\tStage 2: training on dual')\n",
    "    emb_dual_ = get_diff2vec_embeddings(args_d, verbose=0)\n",
    "    emb_dual = np.zeros(shape=(dual_graph.number_of_nodes(), args_d.dimensions))\n",
    "    i2w = emb_dual_.index2word\n",
    "    for w in i2w:\n",
    "        emb_dual[int(w), :] = emb_dual_[w]\n",
    "\n",
    "    edge_embs = emb_dual\n",
    "    enum = {node: num for num, node in enumerate(nodelist)}\n",
    "    num_nodes = adj_mat.shape[0]\n",
    "    node_features = np.zeros((num_nodes, args_d.dimensions))\n",
    "    counts = np.ones(num_nodes)\n",
    "    for i, edge in enumerate(edgelist):\n",
    "        # средние по эмбеддингам рёбер, связанных с данной вершиной\n",
    "        u = enum[edge[0]]; v = enum[edge[1]]\n",
    "        node_features[u, :] += edge_embs[i, :]\n",
    "        node_features[v, :] -= edge_embs[i, :]\n",
    "        counts[u] += 1; counts[v] += 1\n",
    "    node_features /= counts[:, np.newaxis]\n",
    "    \n",
    "    #### return two numpy arrays with features\n",
    "    ## --> maybe also return sp.sparse adjacency matrices\n",
    "    ## --> and labels for the sampled subgraph\n",
    "    ## after that, presumably simply do ->Tensors->Variable (except adj) \n",
    "    ##     and feed into the model\n",
    "    return {'data': (adj_mat, labels, idx_train, idx_val, idx_test, adj_dual),\n",
    "            'features': (emb, node_features, emb_dual)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph has 10312 nodes and 333983 edges\n",
      "Sampling 1000-node subgraph from original graph\n",
      "\n",
      "\tStage 1: training for features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tStage 2: training on dual\n"
     ]
    }
   ],
   "source": [
    "from diff2vec import get_diff2vec_embeddings\n",
    "from copy import copy\n",
    "\n",
    "# path to edgelist\n",
    "dataset_str = '../data/BlogCatalog-dataset/data/blog_catalog.edgelist'\n",
    "\n",
    "n_dim = 16\n",
    "num_epochs = 200\n",
    "seed = 2\n",
    "\n",
    "defaults = {'vertex_set_cardinality': 80, 'num_diffusions': 10,\n",
    "            'workers': 1, 'type': 'eulerian', 'dimensions': n_dim, # set with different values\n",
    "            'alpha': 0.025, 'window_size': 5, 'iter': 2,\n",
    "            'dataset_str': dataset_str}\n",
    "\n",
    "args_p = dotdict(copy(defaults))\n",
    "args_d = dotdict(copy(defaults))\n",
    "\n",
    "\n",
    "dict_res = get_data_with_diffusion_features(args_p, args_d, subgraph={'num': 1000, 'seed': 19}, dataset='bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ->tensors->variables\n",
    "adj, labels, idx_train, idx_val, idx_test, adj_d = dict_res['data']\n",
    "node_features, node_features_from_dual, features_d = dict_res['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_incidence_matrix_EV(adj):\n",
    "    \"\"\"\n",
    "    Make a (V x E) vertex-edge incidence matrix\n",
    "    No masking (multiclass classif)\n",
    "    \"\"\"\n",
    "    edges = nx.from_scipy_sparse_matrix(adj).edges()\n",
    "    row, col = [], []\n",
    "    for edge_id, edge in enumerate(edges):\n",
    "        a, b = edge\n",
    "        col.append(a); row.append(edge_id)\n",
    "        col.append(b); row.append(edge_id)\n",
    "    arr = sp.coo_matrix(([1] * len(row), (row, col)), shape=(len(edges), adj.shape[0]))\n",
    "    return sparse_mx_to_torch_sparse_tensor(arr)\n",
    "\n",
    "commat = make_incidence_matrix_EV(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10312, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj    = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "adj_d  = sparse_mx_to_torch_sparse_tensor(adj_d)\n",
    "labels = torch.LongTensor(labels)\n",
    "labels = labels.view(labels.size(0))\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val   = torch.LongTensor(idx_val)\n",
    "idx_test  = torch.LongTensor(idx_test)\n",
    "\n",
    "# features   = torch.FloatTensor(np.hstack([node_features, node_features_from_dual]))\n",
    "features   = torch.FloatTensor(node_features)\n",
    "features_d = torch.FloatTensor(features_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = Variable(labels)\n",
    "features, features_d = Variable(features), Variable(features_d)\n",
    "commat = Variable(commat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np = labels.data.numpy()\n",
    "edge_labels = Variable(torch.LongTensor( np.array([labels_np[a] == labels_np[b] \n",
    "                                                  for a, b in nx.from_numpy_array(adj.to_dense().numpy()).edges()], dtype='int') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1000]) torch.Size([3487, 3487])\n",
      "torch.Size([1000, 1000])\n",
      "torch.Size([3487, 3487]) torch.Size([3487, 3487])\n"
     ]
    }
   ],
   "source": [
    "print(adj.shape, adj_d.shape)\n",
    "print(features.shape)\n",
    "print(features_d.shape, adj_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Total time elapsed: 69.1271s\n",
      "Test set results: loss= 3.2592 accuracy= 0.1600 f1= 0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksk/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=n_hidden,\n",
    "            nclass=labels.max().data[0] + 1,\n",
    "            dropout=dropout)\n",
    "\n",
    "model_d = GCN(nfeat=features_d.shape[1],\n",
    "             nhid=n_hidden,\n",
    "             nclass=2,\n",
    "             dropout=dropout)\n",
    "\n",
    "# Optimizers\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "optimizer_d = optim.Adam(model_d.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multiclass_classification_combined_loss(output, labels, output_d, edge_labels, approx_err,\n",
    "                                            idx_train, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute negative log-likelihood loss for multiclass task\n",
    "    \"\"\"\n",
    "    orig_loss = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    dual_loss = F.nll_loss(output_d, edge_labels)\n",
    "    \n",
    "    return orig_loss + alpha * dual_loss + beta * approx_err\n",
    "\n",
    "\n",
    "        \n",
    "alpha = 1e-1\n",
    "beta = 1e-2\n",
    "alpha = Variable(torch.FloatTensor([alpha]))\n",
    "beta = Variable(torch.FloatTensor([beta]))\n",
    "\n",
    "\n",
    "def train(epoch, verbose=True):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    model_d.train()\n",
    "    optimizer_d.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    output_d = model_d(features_d, adj_d)\n",
    "\n",
    "    approx_err = ((SparseMM2_GCN(commat)(model.gc1.weight) - model_d.gc1.weight) ** 2).sum()\n",
    "\n",
    "    loss_train = multiclass_classification_combined_loss(output, labels, output_d, edge_labels,\n",
    "                                                        approx_err, idx_train, alpha, beta)\n",
    "\n",
    "    \n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    if verbose:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.data[0]),\n",
    "              'acc_train: {:.4f}'.format(acc_train.data[0]),\n",
    "              'loss_val: {:.4f}'.format(loss_val.data[0]),\n",
    "              'acc_val: {:.4f}'.format(acc_val.data[0]),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    f1_test = f1(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data[0]),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data[0]),\n",
    "          \"f1= {:.4f}\".format(f1_test))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "\n",
    "for epoch in range(200):\n",
    "    train(epoch, verbose=False)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "accuracy 0.1650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment this to replace with dummy features\n",
    "features = Variable(torch.FloatTensor(np.identity(adj.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Total time elapsed: 2.0656s\n",
      "Test set results: loss= 7.5496 accuracy= 0.1400 f1= 0.0371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksk/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Base model and optimizer\n",
    "\n",
    "model_base = GCN(nfeat=features.shape[1],\n",
    "                nhid=n_hidden,\n",
    "                nclass=labels.max().data[0] + 1,\n",
    "                dropout=dropout)\n",
    "\n",
    "optimizer_base = optim.Adam(model_base.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def train_base(epoch, verbose=True):\n",
    "    t = time.time()\n",
    "    model_base.train()\n",
    "    optimizer_base.zero_grad()\n",
    "    output = model_base(features, adj)\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer_base.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model_base.eval()\n",
    "        output = model_base(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    if verbose:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.data[0]),\n",
    "              'acc_train: {:.4f}'.format(acc_train.data[0]),\n",
    "              'loss_val: {:.4f}'.format(loss_val.data[0]),\n",
    "              'acc_val: {:.4f}'.format(acc_val.data[0]),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test_base():\n",
    "    model_base.eval()\n",
    "    output = model_base(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    f1_test = f1(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data[0]),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data[0]),\n",
    "          \"f1= {:.4f}\".format(f1_test))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_base(epoch, verbose=False)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test_base()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* num = 500, seed = 10:\n",
    "\n",
    "0.11 base+dummy features, 0.18 diffusion features and dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original self-contained piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksk/Desktop/sem2/graph_embeddings/MAIN/models.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9504 acc_train: 0.1310 loss_val: 1.9658 acc_val: 0.1100 time: 0.0278s\n",
      "Epoch: 0002 loss_train: 1.9393 acc_train: 0.1320 loss_val: 1.9620 acc_val: 0.1150 time: 0.0209s\n",
      "Epoch: 0003 loss_train: 1.9202 acc_train: 0.2020 loss_val: 1.9424 acc_val: 0.1450 time: 0.0212s\n",
      "Epoch: 0004 loss_train: 1.9105 acc_train: 0.2740 loss_val: 1.9264 acc_val: 0.2600 time: 0.0204s\n",
      "Epoch: 0005 loss_train: 1.8961 acc_train: 0.2970 loss_val: 1.9154 acc_val: 0.2900 time: 0.0175s\n",
      "Epoch: 0006 loss_train: 1.8874 acc_train: 0.3170 loss_val: 1.9025 acc_val: 0.3100 time: 0.0188s\n",
      "Epoch: 0007 loss_train: 1.8727 acc_train: 0.3150 loss_val: 1.8913 acc_val: 0.3050 time: 0.0186s\n",
      "Epoch: 0008 loss_train: 1.8587 acc_train: 0.3140 loss_val: 1.8813 acc_val: 0.3100 time: 0.0167s\n",
      "Epoch: 0009 loss_train: 1.8534 acc_train: 0.3140 loss_val: 1.8714 acc_val: 0.3150 time: 0.0154s\n",
      "Epoch: 0010 loss_train: 1.8335 acc_train: 0.3150 loss_val: 1.8571 acc_val: 0.3100 time: 0.0178s\n",
      "Epoch: 0011 loss_train: 1.8193 acc_train: 0.3230 loss_val: 1.8399 acc_val: 0.3100 time: 0.0196s\n",
      "Epoch: 0012 loss_train: 1.8047 acc_train: 0.3180 loss_val: 1.8218 acc_val: 0.3150 time: 0.0194s\n",
      "Epoch: 0013 loss_train: 1.7898 acc_train: 0.3240 loss_val: 1.8211 acc_val: 0.3150 time: 0.0197s\n",
      "Epoch: 0014 loss_train: 1.7720 acc_train: 0.3290 loss_val: 1.7985 acc_val: 0.3350 time: 0.0199s\n",
      "Epoch: 0015 loss_train: 1.7622 acc_train: 0.3440 loss_val: 1.7956 acc_val: 0.3150 time: 0.0227s\n",
      "Epoch: 0016 loss_train: 1.7381 acc_train: 0.3420 loss_val: 1.7700 acc_val: 0.3350 time: 0.0230s\n",
      "Epoch: 0017 loss_train: 1.7221 acc_train: 0.3460 loss_val: 1.7520 acc_val: 0.3350 time: 0.0196s\n",
      "Epoch: 0018 loss_train: 1.6956 acc_train: 0.3710 loss_val: 1.7356 acc_val: 0.3550 time: 0.0202s\n",
      "Epoch: 0019 loss_train: 1.6845 acc_train: 0.3560 loss_val: 1.7179 acc_val: 0.3450 time: 0.0231s\n",
      "Epoch: 0020 loss_train: 1.6693 acc_train: 0.3600 loss_val: 1.7116 acc_val: 0.3550 time: 0.0206s\n",
      "Epoch: 0021 loss_train: 1.6432 acc_train: 0.3750 loss_val: 1.6855 acc_val: 0.3450 time: 0.0169s\n",
      "Epoch: 0022 loss_train: 1.6146 acc_train: 0.3720 loss_val: 1.6669 acc_val: 0.3400 time: 0.0198s\n",
      "Epoch: 0023 loss_train: 1.6053 acc_train: 0.3630 loss_val: 1.6435 acc_val: 0.3700 time: 0.0196s\n",
      "Epoch: 0024 loss_train: 1.5891 acc_train: 0.3670 loss_val: 1.6292 acc_val: 0.3700 time: 0.0193s\n",
      "Epoch: 0025 loss_train: 1.5677 acc_train: 0.3770 loss_val: 1.6060 acc_val: 0.3700 time: 0.0199s\n",
      "Epoch: 0026 loss_train: 1.5418 acc_train: 0.3710 loss_val: 1.6138 acc_val: 0.3450 time: 0.0226s\n",
      "Epoch: 0027 loss_train: 1.5206 acc_train: 0.3850 loss_val: 1.5774 acc_val: 0.3800 time: 0.0206s\n",
      "Epoch: 0028 loss_train: 1.4953 acc_train: 0.3920 loss_val: 1.5681 acc_val: 0.3650 time: 0.0169s\n",
      "Epoch: 0029 loss_train: 1.4642 acc_train: 0.4030 loss_val: 1.5128 acc_val: 0.4000 time: 0.0171s\n",
      "Epoch: 0030 loss_train: 1.4419 acc_train: 0.4080 loss_val: 1.5180 acc_val: 0.3850 time: 0.0172s\n",
      "Epoch: 0031 loss_train: 1.4057 acc_train: 0.4530 loss_val: 1.4952 acc_val: 0.3850 time: 0.0179s\n",
      "Epoch: 0032 loss_train: 1.3819 acc_train: 0.4770 loss_val: 1.4543 acc_val: 0.4100 time: 0.0197s\n",
      "Epoch: 0033 loss_train: 1.3579 acc_train: 0.5090 loss_val: 1.4291 acc_val: 0.4550 time: 0.0195s\n",
      "Epoch: 0034 loss_train: 1.3289 acc_train: 0.5280 loss_val: 1.4155 acc_val: 0.4800 time: 0.0195s\n",
      "Epoch: 0035 loss_train: 1.3050 acc_train: 0.5580 loss_val: 1.3900 acc_val: 0.4750 time: 0.0198s\n",
      "Epoch: 0036 loss_train: 1.2926 acc_train: 0.5760 loss_val: 1.3890 acc_val: 0.5000 time: 0.0205s\n",
      "Epoch: 0037 loss_train: 1.2580 acc_train: 0.5830 loss_val: 1.3497 acc_val: 0.5300 time: 0.0225s\n",
      "Epoch: 0038 loss_train: 1.2144 acc_train: 0.5990 loss_val: 1.3197 acc_val: 0.5200 time: 0.0200s\n",
      "Epoch: 0039 loss_train: 1.2035 acc_train: 0.6080 loss_val: 1.3111 acc_val: 0.5400 time: 0.0168s\n",
      "Epoch: 0040 loss_train: 1.1981 acc_train: 0.5930 loss_val: 1.3139 acc_val: 0.5350 time: 0.0193s\n",
      "Epoch: 0041 loss_train: 1.1710 acc_train: 0.6290 loss_val: 1.2526 acc_val: 0.5550 time: 0.0185s\n",
      "Epoch: 0042 loss_train: 1.1482 acc_train: 0.6440 loss_val: 1.2582 acc_val: 0.5600 time: 0.0193s\n",
      "Epoch: 0043 loss_train: 1.1235 acc_train: 0.6440 loss_val: 1.2230 acc_val: 0.5950 time: 0.0192s\n",
      "Epoch: 0044 loss_train: 1.1054 acc_train: 0.6300 loss_val: 1.2523 acc_val: 0.5700 time: 0.0205s\n",
      "Epoch: 0045 loss_train: 1.0797 acc_train: 0.6770 loss_val: 1.2100 acc_val: 0.6100 time: 0.0196s\n",
      "Epoch: 0046 loss_train: 1.0444 acc_train: 0.6680 loss_val: 1.1794 acc_val: 0.5750 time: 0.0210s\n",
      "Epoch: 0047 loss_train: 1.0368 acc_train: 0.6840 loss_val: 1.1647 acc_val: 0.6150 time: 0.0248s\n",
      "Epoch: 0048 loss_train: 0.9992 acc_train: 0.7320 loss_val: 1.1603 acc_val: 0.6300 time: 0.0193s\n",
      "Epoch: 0049 loss_train: 0.9987 acc_train: 0.7250 loss_val: 1.1221 acc_val: 0.6750 time: 0.0193s\n",
      "Epoch: 0050 loss_train: 0.9649 acc_train: 0.7410 loss_val: 1.1095 acc_val: 0.6600 time: 0.0172s\n",
      "Epoch: 0051 loss_train: 0.9699 acc_train: 0.7410 loss_val: 1.1368 acc_val: 0.6450 time: 0.0189s\n",
      "Epoch: 0052 loss_train: 0.9248 acc_train: 0.7510 loss_val: 1.0817 acc_val: 0.6800 time: 0.0196s\n",
      "Epoch: 0053 loss_train: 0.9015 acc_train: 0.7560 loss_val: 1.0417 acc_val: 0.7050 time: 0.0190s\n",
      "Epoch: 0054 loss_train: 0.8759 acc_train: 0.8050 loss_val: 1.0675 acc_val: 0.7200 time: 0.0196s\n",
      "Epoch: 0055 loss_train: 0.8786 acc_train: 0.8070 loss_val: 1.0794 acc_val: 0.7000 time: 0.0202s\n",
      "Epoch: 0056 loss_train: 0.8369 acc_train: 0.8240 loss_val: 0.9988 acc_val: 0.7450 time: 0.0307s\n",
      "Epoch: 0057 loss_train: 0.8506 acc_train: 0.8160 loss_val: 1.0197 acc_val: 0.7300 time: 0.0212s\n",
      "Epoch: 0058 loss_train: 0.8278 acc_train: 0.8260 loss_val: 1.0372 acc_val: 0.7350 time: 0.0224s\n",
      "Epoch: 0059 loss_train: 0.7912 acc_train: 0.8410 loss_val: 0.9487 acc_val: 0.7700 time: 0.0186s\n",
      "Epoch: 0060 loss_train: 0.7819 acc_train: 0.8500 loss_val: 0.9622 acc_val: 0.7350 time: 0.0177s\n",
      "Epoch: 0061 loss_train: 0.7653 acc_train: 0.8590 loss_val: 0.9574 acc_val: 0.7800 time: 0.0164s\n",
      "Epoch: 0062 loss_train: 0.7745 acc_train: 0.8420 loss_val: 0.9846 acc_val: 0.7300 time: 0.0210s\n",
      "Epoch: 0063 loss_train: 0.7566 acc_train: 0.8500 loss_val: 0.9531 acc_val: 0.7650 time: 0.0234s\n",
      "Epoch: 0064 loss_train: 0.7225 acc_train: 0.8700 loss_val: 0.9455 acc_val: 0.7950 time: 0.0195s\n",
      "Epoch: 0065 loss_train: 0.7299 acc_train: 0.8730 loss_val: 0.9330 acc_val: 0.7650 time: 0.0206s\n",
      "Epoch: 0066 loss_train: 0.7031 acc_train: 0.8820 loss_val: 0.8647 acc_val: 0.7900 time: 0.0228s\n",
      "Epoch: 0067 loss_train: 0.6844 acc_train: 0.8850 loss_val: 0.9033 acc_val: 0.8000 time: 0.0225s\n",
      "Epoch: 0068 loss_train: 0.6871 acc_train: 0.8820 loss_val: 0.8985 acc_val: 0.7800 time: 0.0193s\n",
      "Epoch: 0069 loss_train: 0.6652 acc_train: 0.8760 loss_val: 0.8907 acc_val: 0.7850 time: 0.0197s\n",
      "Epoch: 0070 loss_train: 0.6507 acc_train: 0.8750 loss_val: 0.8748 acc_val: 0.8100 time: 0.0200s\n",
      "Epoch: 0071 loss_train: 0.6485 acc_train: 0.8810 loss_val: 0.8811 acc_val: 0.7750 time: 0.0214s\n",
      "Epoch: 0072 loss_train: 0.6323 acc_train: 0.8930 loss_val: 0.8196 acc_val: 0.8450 time: 0.0215s\n",
      "Epoch: 0073 loss_train: 0.6088 acc_train: 0.8980 loss_val: 0.8502 acc_val: 0.8200 time: 0.0195s\n",
      "Epoch: 0074 loss_train: 0.5908 acc_train: 0.9050 loss_val: 0.8265 acc_val: 0.8200 time: 0.0197s\n",
      "Epoch: 0075 loss_train: 0.6007 acc_train: 0.8830 loss_val: 0.8261 acc_val: 0.7800 time: 0.0205s\n",
      "Epoch: 0076 loss_train: 0.6048 acc_train: 0.8880 loss_val: 0.8579 acc_val: 0.7850 time: 0.0177s\n",
      "Epoch: 0077 loss_train: 0.5882 acc_train: 0.9060 loss_val: 0.8140 acc_val: 0.8400 time: 0.0187s\n",
      "Epoch: 0078 loss_train: 0.5664 acc_train: 0.8960 loss_val: 0.8135 acc_val: 0.8100 time: 0.0192s\n",
      "Epoch: 0079 loss_train: 0.6097 acc_train: 0.8930 loss_val: 0.8467 acc_val: 0.7800 time: 0.0166s\n",
      "Epoch: 0080 loss_train: 0.5776 acc_train: 0.8980 loss_val: 0.8233 acc_val: 0.8050 time: 0.0160s\n",
      "Epoch: 0081 loss_train: 0.5816 acc_train: 0.8990 loss_val: 0.8443 acc_val: 0.8000 time: 0.0187s\n",
      "Epoch: 0082 loss_train: 0.5389 acc_train: 0.9020 loss_val: 0.8046 acc_val: 0.8250 time: 0.0192s\n",
      "Epoch: 0083 loss_train: 0.5351 acc_train: 0.9190 loss_val: 0.8283 acc_val: 0.8050 time: 0.0197s\n",
      "Epoch: 0084 loss_train: 0.5664 acc_train: 0.9070 loss_val: 0.8561 acc_val: 0.7950 time: 0.0195s\n",
      "Epoch: 0085 loss_train: 0.5454 acc_train: 0.9110 loss_val: 0.8155 acc_val: 0.8200 time: 0.0206s\n",
      "Epoch: 0086 loss_train: 0.5209 acc_train: 0.9010 loss_val: 0.7889 acc_val: 0.8100 time: 0.0186s\n",
      "Epoch: 0087 loss_train: 0.5588 acc_train: 0.9150 loss_val: 0.8121 acc_val: 0.7900 time: 0.0190s\n",
      "Epoch: 0088 loss_train: 0.5129 acc_train: 0.9140 loss_val: 0.7268 acc_val: 0.8400 time: 0.0175s\n",
      "Epoch: 0089 loss_train: 0.5124 acc_train: 0.9180 loss_val: 0.8049 acc_val: 0.8300 time: 0.0196s\n",
      "Epoch: 0090 loss_train: 0.5229 acc_train: 0.9040 loss_val: 0.7796 acc_val: 0.8100 time: 0.0187s\n",
      "Epoch: 0091 loss_train: 0.5084 acc_train: 0.9070 loss_val: 0.8011 acc_val: 0.8200 time: 0.0195s\n",
      "Epoch: 0092 loss_train: 0.4801 acc_train: 0.9260 loss_val: 0.7841 acc_val: 0.8000 time: 0.0181s\n",
      "Epoch: 0093 loss_train: 0.5036 acc_train: 0.9140 loss_val: 0.7789 acc_val: 0.8250 time: 0.0197s\n",
      "Epoch: 0094 loss_train: 0.4778 acc_train: 0.9230 loss_val: 0.7209 acc_val: 0.8250 time: 0.0202s\n",
      "Epoch: 0095 loss_train: 0.4778 acc_train: 0.9180 loss_val: 0.7783 acc_val: 0.8050 time: 0.0194s\n",
      "Epoch: 0096 loss_train: 0.4888 acc_train: 0.9160 loss_val: 0.7683 acc_val: 0.8150 time: 0.0195s\n",
      "Epoch: 0097 loss_train: 0.4996 acc_train: 0.9120 loss_val: 0.7747 acc_val: 0.8050 time: 0.0192s\n",
      "Epoch: 0098 loss_train: 0.4765 acc_train: 0.9190 loss_val: 0.6968 acc_val: 0.8350 time: 0.0197s\n",
      "Epoch: 0099 loss_train: 0.4830 acc_train: 0.9140 loss_val: 0.7817 acc_val: 0.8050 time: 0.0179s\n",
      "Epoch: 0100 loss_train: 0.4850 acc_train: 0.9160 loss_val: 0.7436 acc_val: 0.8200 time: 0.0159s\n",
      "Epoch: 0101 loss_train: 0.4688 acc_train: 0.9170 loss_val: 0.7586 acc_val: 0.8150 time: 0.0172s\n",
      "Epoch: 0102 loss_train: 0.4749 acc_train: 0.9270 loss_val: 0.7765 acc_val: 0.7900 time: 0.0162s\n",
      "Epoch: 0103 loss_train: 0.4982 acc_train: 0.9200 loss_val: 0.8041 acc_val: 0.7800 time: 0.0192s\n",
      "Epoch: 0104 loss_train: 0.4526 acc_train: 0.9230 loss_val: 0.7317 acc_val: 0.8250 time: 0.0196s\n",
      "Epoch: 0105 loss_train: 0.4652 acc_train: 0.9240 loss_val: 0.7722 acc_val: 0.7950 time: 0.0193s\n",
      "Epoch: 0106 loss_train: 0.4723 acc_train: 0.9110 loss_val: 0.7508 acc_val: 0.8250 time: 0.0197s\n",
      "Epoch: 0107 loss_train: 0.4630 acc_train: 0.9260 loss_val: 0.7550 acc_val: 0.8150 time: 0.0246s\n",
      "Epoch: 0108 loss_train: 0.4424 acc_train: 0.9350 loss_val: 0.7767 acc_val: 0.8050 time: 0.0239s\n",
      "Epoch: 0109 loss_train: 0.4354 acc_train: 0.9330 loss_val: 0.6971 acc_val: 0.8350 time: 0.0266s\n",
      "Epoch: 0110 loss_train: 0.4307 acc_train: 0.9250 loss_val: 0.7506 acc_val: 0.8050 time: 0.0211s\n",
      "Epoch: 0111 loss_train: 0.4419 acc_train: 0.9290 loss_val: 0.7044 acc_val: 0.8050 time: 0.0217s\n",
      "Epoch: 0112 loss_train: 0.4420 acc_train: 0.9340 loss_val: 0.7377 acc_val: 0.7950 time: 0.0215s\n",
      "Epoch: 0113 loss_train: 0.4309 acc_train: 0.9370 loss_val: 0.7770 acc_val: 0.7900 time: 0.0206s\n",
      "Epoch: 0114 loss_train: 0.4503 acc_train: 0.9290 loss_val: 0.7323 acc_val: 0.8250 time: 0.0228s\n",
      "Epoch: 0115 loss_train: 0.4425 acc_train: 0.9290 loss_val: 0.7125 acc_val: 0.8300 time: 0.0216s\n",
      "Epoch: 0116 loss_train: 0.4224 acc_train: 0.9380 loss_val: 0.6695 acc_val: 0.8250 time: 0.0204s\n",
      "Epoch: 0117 loss_train: 0.4333 acc_train: 0.9290 loss_val: 0.7011 acc_val: 0.8400 time: 0.0187s\n",
      "Epoch: 0118 loss_train: 0.4201 acc_train: 0.9350 loss_val: 0.7500 acc_val: 0.8100 time: 0.0207s\n",
      "Epoch: 0119 loss_train: 0.4365 acc_train: 0.9170 loss_val: 0.7537 acc_val: 0.8000 time: 0.0207s\n",
      "Epoch: 0120 loss_train: 0.4355 acc_train: 0.9290 loss_val: 0.7174 acc_val: 0.8250 time: 0.0200s\n",
      "Epoch: 0121 loss_train: 0.4184 acc_train: 0.9340 loss_val: 0.7584 acc_val: 0.8200 time: 0.0214s\n",
      "Epoch: 0122 loss_train: 0.4151 acc_train: 0.9290 loss_val: 0.7034 acc_val: 0.8200 time: 0.0196s\n",
      "Epoch: 0123 loss_train: 0.4098 acc_train: 0.9310 loss_val: 0.7190 acc_val: 0.8300 time: 0.0189s\n",
      "Epoch: 0124 loss_train: 0.4208 acc_train: 0.9290 loss_val: 0.8083 acc_val: 0.8100 time: 0.0172s\n",
      "Epoch: 0125 loss_train: 0.4164 acc_train: 0.9260 loss_val: 0.7164 acc_val: 0.8450 time: 0.0185s\n",
      "Epoch: 0126 loss_train: 0.4306 acc_train: 0.9260 loss_val: 0.7715 acc_val: 0.8100 time: 0.0156s\n",
      "Epoch: 0127 loss_train: 0.3989 acc_train: 0.9260 loss_val: 0.7190 acc_val: 0.8300 time: 0.0171s\n",
      "Epoch: 0128 loss_train: 0.4106 acc_train: 0.9390 loss_val: 0.7150 acc_val: 0.8150 time: 0.0186s\n",
      "Epoch: 0129 loss_train: 0.4216 acc_train: 0.9330 loss_val: 0.7770 acc_val: 0.8300 time: 0.0188s\n",
      "Epoch: 0130 loss_train: 0.4111 acc_train: 0.9330 loss_val: 0.7182 acc_val: 0.8300 time: 0.0159s\n",
      "Epoch: 0131 loss_train: 0.3930 acc_train: 0.9450 loss_val: 0.6686 acc_val: 0.8600 time: 0.0162s\n",
      "Epoch: 0132 loss_train: 0.4040 acc_train: 0.9300 loss_val: 0.7275 acc_val: 0.8100 time: 0.0166s\n",
      "Epoch: 0133 loss_train: 0.3889 acc_train: 0.9380 loss_val: 0.7218 acc_val: 0.8400 time: 0.0192s\n",
      "Epoch: 0134 loss_train: 0.4059 acc_train: 0.9420 loss_val: 0.7036 acc_val: 0.8200 time: 0.0196s\n",
      "Epoch: 0135 loss_train: 0.3740 acc_train: 0.9420 loss_val: 0.6837 acc_val: 0.8150 time: 0.0196s\n",
      "Epoch: 0136 loss_train: 0.3963 acc_train: 0.9390 loss_val: 0.7046 acc_val: 0.8150 time: 0.0195s\n",
      "Epoch: 0137 loss_train: 0.3837 acc_train: 0.9390 loss_val: 0.7292 acc_val: 0.8100 time: 0.0184s\n",
      "Epoch: 0138 loss_train: 0.3791 acc_train: 0.9430 loss_val: 0.6917 acc_val: 0.8100 time: 0.0184s\n",
      "Epoch: 0139 loss_train: 0.3807 acc_train: 0.9340 loss_val: 0.7016 acc_val: 0.8200 time: 0.0174s\n",
      "Epoch: 0140 loss_train: 0.3823 acc_train: 0.9360 loss_val: 0.7039 acc_val: 0.8250 time: 0.0146s\n",
      "Epoch: 0141 loss_train: 0.3907 acc_train: 0.9340 loss_val: 0.7311 acc_val: 0.8050 time: 0.0195s\n",
      "Epoch: 0142 loss_train: 0.3895 acc_train: 0.9310 loss_val: 0.7799 acc_val: 0.8150 time: 0.0169s\n",
      "Epoch: 0143 loss_train: 0.3976 acc_train: 0.9300 loss_val: 0.7243 acc_val: 0.8200 time: 0.0188s\n",
      "Epoch: 0144 loss_train: 0.3847 acc_train: 0.9270 loss_val: 0.6771 acc_val: 0.8000 time: 0.0195s\n",
      "Epoch: 0145 loss_train: 0.3732 acc_train: 0.9340 loss_val: 0.7039 acc_val: 0.8250 time: 0.0191s\n",
      "Epoch: 0146 loss_train: 0.3812 acc_train: 0.9260 loss_val: 0.6684 acc_val: 0.8300 time: 0.0188s\n",
      "Epoch: 0147 loss_train: 0.3921 acc_train: 0.9320 loss_val: 0.7159 acc_val: 0.8150 time: 0.0187s\n",
      "Epoch: 0148 loss_train: 0.3701 acc_train: 0.9340 loss_val: 0.6973 acc_val: 0.8350 time: 0.0181s\n",
      "Epoch: 0149 loss_train: 0.3517 acc_train: 0.9520 loss_val: 0.6782 acc_val: 0.8350 time: 0.0179s\n",
      "Epoch: 0150 loss_train: 0.3602 acc_train: 0.9450 loss_val: 0.6659 acc_val: 0.8400 time: 0.0193s\n",
      "Epoch: 0151 loss_train: 0.3669 acc_train: 0.9390 loss_val: 0.6336 acc_val: 0.8300 time: 0.0195s\n",
      "Epoch: 0152 loss_train: 0.3689 acc_train: 0.9390 loss_val: 0.6717 acc_val: 0.8450 time: 0.0192s\n",
      "Epoch: 0153 loss_train: 0.3689 acc_train: 0.9430 loss_val: 0.6820 acc_val: 0.8400 time: 0.0188s\n",
      "Epoch: 0154 loss_train: 0.3691 acc_train: 0.9420 loss_val: 0.6767 acc_val: 0.8300 time: 0.0187s\n",
      "Epoch: 0155 loss_train: 0.3791 acc_train: 0.9370 loss_val: 0.6451 acc_val: 0.8600 time: 0.0189s\n",
      "Epoch: 0156 loss_train: 0.3933 acc_train: 0.9350 loss_val: 0.7476 acc_val: 0.8100 time: 0.0194s\n",
      "Epoch: 0157 loss_train: 0.3739 acc_train: 0.9390 loss_val: 0.7203 acc_val: 0.8000 time: 0.0235s\n",
      "Epoch: 0158 loss_train: 0.3579 acc_train: 0.9360 loss_val: 0.6560 acc_val: 0.8150 time: 0.0245s\n",
      "Epoch: 0159 loss_train: 0.3624 acc_train: 0.9410 loss_val: 0.6969 acc_val: 0.7950 time: 0.0221s\n",
      "Epoch: 0160 loss_train: 0.3469 acc_train: 0.9420 loss_val: 0.6456 acc_val: 0.8350 time: 0.0225s\n",
      "Epoch: 0161 loss_train: 0.3609 acc_train: 0.9450 loss_val: 0.6824 acc_val: 0.8350 time: 0.0238s\n",
      "Epoch: 0162 loss_train: 0.3675 acc_train: 0.9340 loss_val: 0.7300 acc_val: 0.8300 time: 0.0162s\n",
      "Epoch: 0163 loss_train: 0.3630 acc_train: 0.9330 loss_val: 0.6631 acc_val: 0.8200 time: 0.0139s\n",
      "Epoch: 0164 loss_train: 0.3457 acc_train: 0.9470 loss_val: 0.7130 acc_val: 0.8100 time: 0.0184s\n",
      "Epoch: 0165 loss_train: 0.3238 acc_train: 0.9430 loss_val: 0.5987 acc_val: 0.8600 time: 0.0193s\n",
      "Epoch: 0166 loss_train: 0.3646 acc_train: 0.9370 loss_val: 0.7165 acc_val: 0.8050 time: 0.0195s\n",
      "Epoch: 0167 loss_train: 0.3636 acc_train: 0.9300 loss_val: 0.6545 acc_val: 0.8250 time: 0.0191s\n",
      "Epoch: 0168 loss_train: 0.3406 acc_train: 0.9500 loss_val: 0.6813 acc_val: 0.8350 time: 0.0173s\n",
      "Epoch: 0169 loss_train: 0.3502 acc_train: 0.9370 loss_val: 0.6671 acc_val: 0.8200 time: 0.0188s\n",
      "Epoch: 0170 loss_train: 0.3498 acc_train: 0.9400 loss_val: 0.6608 acc_val: 0.8350 time: 0.0169s\n",
      "Epoch: 0171 loss_train: 0.3495 acc_train: 0.9440 loss_val: 0.6957 acc_val: 0.8050 time: 0.0178s\n",
      "Epoch: 0172 loss_train: 0.3500 acc_train: 0.9320 loss_val: 0.7105 acc_val: 0.8300 time: 0.0167s\n",
      "Epoch: 0173 loss_train: 0.3442 acc_train: 0.9410 loss_val: 0.6791 acc_val: 0.8350 time: 0.0193s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-259067e215e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mt_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimization Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time elapsed: {:.4f}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-259067e215e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksk/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksk/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# adj, features, labels, idx_train, idx_val, idx_test = load_multiclass(path='../data/BlogCatalog-dataset/data/',\n",
    "#                                                                       dataset='blog_catalog')\n",
    "\n",
    "\n",
    "del GCN\n",
    "from models import GCN\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_multiclass()\n",
    "features = torch.FloatTensor(np.identity(adj.shape[0]))\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=n_hidden,\n",
    "            nclass=labels.max() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "features, labels = Variable(features), Variable(labels)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data[0]),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data[0]),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data[0]),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data[0]),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data[0]),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data[0]))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(500):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without node features:\n",
    "\n",
    "Accuracy (6-class?) 0.66 this (300 epochs) vs 0.61 baseline (200 epochs)\n",
    "\n",
    "with node features about the same (weaker data then auxilary information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки\n",
    "\n",
    "\n",
    "* [PyGCN](https://github.com/tkipf/pygcn/tree/master/pygcn)\n",
    "\n",
    "* [GAE in Pytorch](https://github.com/vmasrani/gae_in_pytorch/blob/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trashbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def make_common_matrix(adj, edges, idx_train):\n",
    "#     print('Starting')\n",
    "#     arr = np.zeros(shape=(adj.shape[0], len(edges)))\n",
    "#     print('Matrix created')\n",
    "#     for edge_id in edges:\n",
    "#         a, b = edges[edge_id]\n",
    "#         if a not in idx_train or b not in idx_train:\n",
    "#             continue\n",
    "#         arr[a, edge_id] = 1\n",
    "#         arr[b, edge_id] = 1\n",
    "#     print('Returning')\n",
    "#     return torch.FloatTensor(arr)\n",
    "\n",
    "\n",
    "\n",
    "## Previous version:\n",
    "# gr = nx.from_scipy_sparse_matrix(adj)\n",
    "# sub_nodes = np.random.choice(gr.nodes(), replace=0, size=int(len(gr.nodes()) * 0.4))\n",
    "# subgraph = gr.subgraph(sub_nodes)\n",
    "# adj_d, edges = get_dual(subgraph, sparse=False)\n",
    "# edges = {i: n for i, n in enumerate(edges)}\n",
    "# adj_d = torch.FloatTensor(adj_d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if unsupervised, also use a decoder to compute loss\n",
    "# decoder = InnerProductDecoder(dropout)\n",
    "# def unsupervised_combined_loss(output, output_d, approx_err,\n",
    "#                                alpha, beta):\n",
    "#     \"\"\"\n",
    "#     Compute negative log-likelihood loss for multiclass task\n",
    "#     \"\"\"\n",
    "#     recovered_adj = decoder(output)\n",
    "#     recovered_adj_d = decoder(output_d)\n",
    "#     orig_loss = (SparseDiff_GCN(adj)(recovered_adj) ** 2).sum()\n",
    "#     dual_loss = (SparseDiff_GCN(adj_d)(recovered_adj_d) ** 2).sum()\n",
    "\n",
    "#     return orig_loss + alpha * dual_loss + beta * approx_err\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + \\\n",
    "    #             alpha * F.nll_loss(output_d, edge_labels) + \\\n",
    "    #             beta * approx_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import math\n",
    "# import torch\n",
    "# from torch.nn.parameter import Parameter\n",
    "# from torch.nn.modules.module import Module\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# from utils import get_subsampler, make_sparse\n",
    "# from utils import eval_gae_lp, plot_results\n",
    "# from preprocessing import preprocess_train_test_split\n",
    "\n",
    "# from dist import weighted_bernoulli\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "\n",
    "# class SparseMM_GCN(torch.autograd.Function):\n",
    "#     \"\"\"\n",
    "#     Sparse x dense matrix multiplication with autograd support.\n",
    "\n",
    "#     Implementation by Soumith Chintala:\n",
    "#     https://discuss.pytorch.org/t/\n",
    "#     does-pytorch-support-autograd-on-sparse-matrix/6156/7\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, sparse):\n",
    "#         super(SparseMM_GCN, self).__init__()\n",
    "#         self.sparse = sparse\n",
    "\n",
    "#     def forward(self, dense):\n",
    "#         return torch.mm(self.sparse, dense)\n",
    "\n",
    "#     def backward(self, grad_output):\n",
    "#         grad_input = None\n",
    "#         if self.needs_input_grad[0]:\n",
    "#             grad_input = torch.mm(self.sparse.t(), grad_output)\n",
    "#         return grad_input\n",
    "\n",
    "\n",
    "class SparseMM2_GCN(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Sparse x dense matrix multiplication with autograd support.\n",
    "\n",
    "    Implementation by Soumith Chintala:\n",
    "    https://discuss.pytorch.org/t/\n",
    "    does-pytorch-support-autograd-on-sparse-matrix/6156/7\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sparse):\n",
    "        super(SparseMM2_GCN, self).__init__()\n",
    "        self.sparse = sparse\n",
    "\n",
    "    def forward(self, dense):\n",
    "        return torch.mm(self.sparse.data, dense)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = None\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = torch.mm(self.sparse.data.t(), grad_output)\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# class SparseDiff_GCN(torch.autograd.Function):\n",
    "#     \"\"\"\n",
    "#     Sparse x dense matrix substraction with autograd support.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, sparse):\n",
    "#         super(SparseDiff_GCN, self).__init__()\n",
    "#         self.sparse = sparse\n",
    "\n",
    "#     def forward(self, dense):\n",
    "#         return self.sparse.to_dense() - dense\n",
    "\n",
    "#     def backward(self, grad_output):\n",
    "#         grad_input = None\n",
    "#         if self.needs_input_grad[0]:\n",
    "#             grad_input = self.sparse.to_dense().t() - grad_output\n",
    "#         return grad_input\n",
    "\n",
    "\n",
    "# class GraphConvolution_GCN(Module):\n",
    "#     \"\"\"\n",
    "#     Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, in_features, out_features, bias=True):\n",
    "#         super(GraphConvolution_GCN, self).__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.out_features = out_features\n",
    "#         self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "#         if bias:\n",
    "#             self.bias = Parameter(torch.Tensor(out_features))\n",
    "#         else:\n",
    "#             self.register_parameter('bias', None)\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "#         if self.bias is not None:\n",
    "#             self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "#     def forward(self, input=None, adj=None, use_input=True):\n",
    "#         if not use_input:\n",
    "#             support = self.weight\n",
    "#         else:\n",
    "#             support = torch.mm(input, self.weight)\n",
    "#         output = SparseMM_GCN(adj)(support)\n",
    "#         if self.bias is not None:\n",
    "#             return output + self.bias\n",
    "#         else:\n",
    "#             return output\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + ' (' \\\n",
    "#                + str(self.in_features) + ' -> ' \\\n",
    "#                + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "# class InnerProductDecoder(Module):\n",
    "#     \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "#     def __init__(self, dropout):\n",
    "#         super(InnerProductDecoder, self).__init__()\n",
    "#         self.dropout = dropout\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.fudge = 1e-7\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         z = F.dropout(z, self.dropout, training=self.training)\n",
    "#         adj = (self.sigmoid(torch.mm(z, z.t())) + self.fudge) * (1 - 2 * self.fudge)\n",
    "#         return adj\n",
    "\n",
    "\n",
    "# ########################################################################################\n",
    "# #                                                                                      #\n",
    "# #######################   GCN - graph convolutional network   ##########################\n",
    "# #                                                                                      #\n",
    "# ########################################################################################\n",
    "\n",
    "# class GCN(Module):\n",
    "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "#         super(GCN, self).__init__()\n",
    "\n",
    "#         self.gc1 = GraphConvolution_GCN(nfeat, nhid)\n",
    "#         self.gc2 = GraphConvolution_GCN(nhid, nclass)\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#     def forward(self, adj):\n",
    "#         # x - feature matrix?\n",
    "#         x = F.relu(self.gc1(adj=adj, use_input=False))\n",
    "#         x = F.dropout(x, self.dropout, training=self.training)\n",
    "#         x = self.gc2(x, adj)\n",
    "#         return F.log_softmax(x, dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "4ffe0ce871ef4a0ba8f01750c8b5346d": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
